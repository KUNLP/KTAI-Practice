{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F24pbMKw8uT9"
   },
   "source": [
    "<h2>개인 구글 드라이브와 colab 연동</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31075.0,
     "status": "ok",
     "timestamp": 1.683128221552E12,
     "user": {
      "displayName": "di wi",
      "userId": "03647248918916298904"
     },
     "user_tz": -540.0
    },
    "id": "4X-lNQfT1D1v",
    "outputId": "2c5c2b26-2bac-4dd8-dca5-04119f88b135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/gdrive\", force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v75mA1w3Ti5H"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FVnmDn36Gh2D",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.68312912555E12,
     "user_tz": -540.0,
     "elapsed": 1523.0,
     "user": {
      "displayName": "di wi",
      "userId": "03647248918916298904"
     }
    },
    "outputId": "de1709e3-fb98-45a0-fe65-57ee233c0b94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "root_dir = \"/gdrive/MyDrive/9. Semantic Analysis\"\n",
    "#root_dir = \"/gdrive/MyDrive/9-2. Semantic Analysis\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfWZLECjoJkn"
   },
   "source": [
    "<h2>WSD 모델</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "o5861idd25QB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.683129140056E12,
     "user_tz": -540.0,
     "elapsed": 7143.0,
     "user": {
      "displayName": "di wi",
      "userId": "03647248918916298904"
     }
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "\n",
    "class WSD(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(WSD, self).__init__(config)\n",
    "\n",
    "        # BERT 모델\n",
    "        self.bert = BertModel(config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # BERT 출력 (batch_size, max_length, hidden_size)\n",
    "        bert_output = outputs[0]\n",
    "\n",
    "        return bert_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caSL4RyA1OOm"
   },
   "source": [
    "<h2>데이터 읽고 전처리 하기</h2>\n",
    "\n",
    "<pre>\n",
    "<b>1. read_data(file_path)</b>\n",
    "  \"datas.txt\" 파일을 읽기 위한 함수\n",
    "  \n",
    "  데이터 예시)\n",
    "    토큰들로 구성된 토큰 시퀀스 \\t \"배\"에 대응하는 토큰 인덱스\n",
    "\n",
    "    ▁보기 만 ▁해도 ▁배 가 ▁부르 다 .\t3\n",
    "    ▁점 심을 ▁먹 지 ▁못해 ▁배 가 ▁많이 ▁고 팠 다 .\t5\n",
    "    ▁배 ▁한 ▁ 척 이 ▁바다 ▁한 가 운 데 ▁떠 ▁있다 .\t0\n",
    "    ▁그 ▁섬 에는 ▁하루 에 ▁두 ▁번 씩 ▁배 가 ▁들어 온 다 .\t8\n",
    "    ▁나는 ▁ 과 일 ▁중 에서 ▁배 를 ▁가장 ▁좋아 한다 .\t6\n",
    "    ▁사 각 사 각 ▁ 씹 히 는 ▁배 의 ▁맛 이 ▁달 고 ▁시 원 하다 .\t8\n",
    "  \n",
    "  read_data(file_path)\n",
    "  args\n",
    "    file_path : 읽고자 하는 데이터의 경로\n",
    "  return\n",
    "    토큰 sequence, \"배\"에 대응하는 토큰 인덱스를 담고 있는 리스트\n",
    "    \n",
    "    출력 예시)\n",
    "      datas = [ (['▁보기', '만', '▁해도', '▁배', '가', '▁부르', '다', '.'], 3)\n",
    "\n",
    "                (...),\n",
    "        \n",
    "              ]\n",
    "      \n",
    "<b>2. convert_data2feature(datas, max_length, tokenizer)</b>\n",
    "  입력 데이터를 indexing 한 후, padding 추가\n",
    "  Tensor로 변환\n",
    "   \n",
    "  convert_data2feature(datas, max_length, tokenizer)\n",
    "  args\n",
    "    datas : 토큰 sequence, \"배\"에 대응하는 토큰 인덱스를 담고 있는 리스트\n",
    "    max_length : 입력의 최대 길이\n",
    "    tokenizer : BERT 토크나이저\n",
    "  return\n",
    "    input_ids_features : 입력 문장에 대한 index sequence\n",
    "    attention_mask_features : padding을 제외한 나머지 실제 토큰 정보를 갖고 있는 sequence\n",
    "    \n",
    "  전처리 예시)\n",
    "    tokens : ['[CLS]', '▁보기', '만', '▁해도', '▁배', '가', '▁부르', '다', '.', '[SEP]']\n",
    "    input_ids : [2, 2362, 6150, 5002, 2287, 5330, 2432, 5782, 54, 3, ...]\n",
    "    attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...]\n",
    " </pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iOGS8rse1ZZZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.683129140057E12,
     "user_tz": -540.0,
     "elapsed": 3.0,
     "user": {
      "displayName": "di wi",
      "userId": "03647248918916298904"
     }
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def read_data(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf8\") as inFile:\n",
    "        lines = inFile.readlines()\n",
    "\n",
    "    datas = []\n",
    "    for line in lines:\n",
    "        # 입력 데이터를 \\t을 기준으로 분리\n",
    "        pieces = line.strip().split(\"\\t\")\n",
    "\n",
    "        # 입력 토큰 시퀀스, 목표 토큰 인덱스\n",
    "        token_sequence, target_token_index = pieces[0].split(\" \"), int(pieces[1])\n",
    "\n",
    "        datas.append((token_sequence, target_token_index))\n",
    "    return datas\n",
    "\n",
    "\n",
    "def convert_data2feature(datas, max_length, tokenizer):\n",
    "    input_ids_features, attention_mask_features = [], []\n",
    "\n",
    "    for token_sequence, target_token_index in datas:\n",
    "\n",
    "        # CLS, SEP 토큰 추가\n",
    "        tokens = [tokenizer.cls_token]\n",
    "        tokens += token_sequence\n",
    "        tokens = tokens[:max_length - 1]\n",
    "        tokens += [tokenizer.sep_token]\n",
    "\n",
    "        # word piece들을 대응하는 index로 치환\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # padding을 제외한 실제 데이터 정보를 반영해주기 위한 attention mask\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # padding 생성\n",
    "        padding = [tokenizer._convert_token_to_id(tokenizer.pad_token)] * (max_length - len(input_ids))\n",
    "        padding_for_mask = [0] * (max_length - len(input_ids))\n",
    "\n",
    "        # padding 추가\n",
    "        input_ids += padding\n",
    "        attention_mask += padding_for_mask\n",
    "\n",
    "        # 변환한 데이터를 각 리스트에 저장\n",
    "        input_ids_features.append(input_ids)\n",
    "        attention_mask_features.append(attention_mask)\n",
    "\n",
    "    # 변환한 데이터를 Tensor 객체에 담아 반환\n",
    "    input_ids_features = torch.tensor(input_ids_features, dtype=torch.long)\n",
    "    attention_mask_features = torch.tensor(attention_mask_features, dtype=torch.long)\n",
    "\n",
    "    return input_ids_features, attention_mask_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urLIOIHT779c"
   },
   "source": [
    "<h2>WSD 모델을 사용하여 문맥에 따라 변하는 단어 벡터 확인</h2>\n",
    "\n",
    "<pre>\n",
    "<b>1. read_data(file_path) 함수를 사용하여 입력 데이터 읽기</b>\n",
    "\n",
    "<b>2. convert_data2feature(datas, max_length, tokenizer) 함수를 사용하여 데이터 전처리</b>\n",
    "\n",
    "<b>3. WSD 모델을 활용하여 \"배\"에 대응하는 벡터 표현 추출</b>\n",
    "\n",
    "<b>4. 서로 다른 문장에서 사용된 \"배\"에 대응하는 벡터 표현 사이의 유사도 비교</b>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rsYLc2YK8eNc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.683129142528E12,
     "user_tz": -540.0,
     "elapsed": 1.0,
     "user": {
      "displayName": "di wi",
      "userId": "03647248918916298904"
     }
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertConfig\n",
    "from tokenization_kobert import KoBertTokenizer\n",
    "\n",
    "def get_cos_sim(vector_1, vector_2):\n",
    "  return np.dot(vector_1, vector_2)/(np.linalg.norm(vector_1)*np.linalg.norm(vector_2))\n",
    "\n",
    "\n",
    "def test(config):\n",
    "    # BERT config 객체 생성\n",
    "    bert_config = BertConfig.from_pretrained(pretrained_model_name_or_path=config[\"pretrained_model_name_or_path\"],\n",
    "                                             cache_dir=config[\"cache_dir_path\"])\n",
    "\n",
    "    # BERT tokenizer 객체 생성\n",
    "    bert_tokenizer = KoBertTokenizer.from_pretrained(pretrained_model_name_or_path=config[\"pretrained_model_name_or_path\"],\n",
    "                                                     cache_dir=config[\"cache_dir_path\"])\n",
    "\n",
    "    # 데이터 읽기\n",
    "    datas = read_data(file_path=config[\"data_path\"])\n",
    "\n",
    "    # 데이터 전처리\n",
    "    input_ids_features, attention_mask_features = convert_data2feature(datas=datas,\n",
    "                                                                       max_length=config[\"max_length\"],\n",
    "                                                                       tokenizer=bert_tokenizer)\n",
    "\n",
    "    # 사전 학습된 BERT 모델 파일로부터 가중치 불러옴\n",
    "    model = WSD.from_pretrained(pretrained_model_name_or_path=config[\"pretrained_model_name_or_path\"],\n",
    "                                cache_dir=config[\"cache_dir_path\"], config=bert_config).cuda()\n",
    "\n",
    "    input_ids_features = input_ids_features.cuda()\n",
    "    attention_mask_features = attention_mask_features.cuda()\n",
    "\n",
    "    # 모델 예측 결과\n",
    "    bert_outputs = model(input_ids_features, attention_mask_features)\n",
    "\n",
    "    input_ids_features = input_ids_features.cpu().detach().numpy().tolist()\n",
    "    bert_outputs = bert_outputs.cpu().detach().numpy().tolist()\n",
    "\n",
    "    # batch 단위로 구성되어 있어 반복문을 통해 하나씩 확인\n",
    "    word2vec = {}\n",
    "    for batch_index in range(len(bert_outputs)):\n",
    "        input_tokens = bert_tokenizer.convert_ids_to_tokens(input_ids_features[batch_index])\n",
    "        bert_output = bert_outputs[batch_index]\n",
    "\n",
    "        token_sequence, target_token_index = datas[batch_index]\n",
    "\n",
    "        # 입력 토큰 시퀀스 문장으로 변환\n",
    "        # ['▁보기', '만', '▁해도', '▁배', '가', '▁부르', '다', '.'] -> 보기만 해도 배가 부르다.\n",
    "        sentence = bert_tokenizer.convert_tokens_to_string(token_sequence)\n",
    "\n",
    "        # token_sequence 앞에 [CLS] 토큰이 추가되었기 때문에 1 추가\n",
    "        target_token_index += 1\n",
    "\n",
    "        target_token = input_tokens[target_token_index]\n",
    "        # 토큰을 단어로 변경 (_배 -> 배)\n",
    "        target_word = bert_tokenizer.convert_tokens_to_string([target_token])\n",
    "        target_vector = bert_output[target_token_index]\n",
    "        # 단어, 단어가 사용된 batch_index, 단어가 사용된 문장\n",
    "        # 배_1 (보기만 해도 배가 부르다.)\n",
    "        word2vec[\"{}_{} ({})\".format(target_word, batch_index+1, sentence)] = target_vector\n",
    "\n",
    "    # \"배\"에 대응하는 각 벡터 표현들 사이의 유사도 계산\n",
    "    word_similarity = {}\n",
    "    for word_1, vector_1 in word2vec.items():\n",
    "\n",
    "        # word_1과 나머지 단어들 사이의 유사도를 담을 리스트 생성\n",
    "        word_similarity[word_1] = []\n",
    "        for word_2, vector_2 in word2vec.items():\n",
    "\n",
    "            # word1과 word2 사이의 코사인 유사도를 계산한 후, 그 결과를 word_similarity 딕셔너리에 저장\n",
    "            # word_similarity의 key : word1, value : (word2, 유사도)\n",
    "            if word_1 == word_2:\n",
    "                continue\n",
    "            cos_sim = get_cos_sim(vector_1 = vector_1, vector_2 = vector_2)\n",
    "            word_similarity[word_1].append((word_2,cos_sim))\n",
    "\n",
    "    print(\"\\n단어1 (단어1이 사용된 문장) vs 단어2 (단어2가 사용된 문장) -> 유사도\\n\")\n",
    "    for word in word_similarity.keys():\n",
    "\n",
    "        # 유사도를 기준으로 정렬, reverse=True를 통해 내림차순으로 정렬\n",
    "        word_similarity[word] = sorted(word_similarity[word], key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "        for index in range(len(word_similarity[word])):\n",
    "            print(\"{} vs {} -> {}\".format(word, word_similarity[word][index][0], round(word_similarity[word][index][1], 4)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21541.0,
     "status": "ok",
     "timestamp": 1.68312916682E12,
     "user": {
      "displayName": "di wi",
      "userId": "03647248918916298904"
     },
     "user_tz": -540.0
    },
    "id": "GbtyjwvtFxf7",
    "outputId": "faad54cf-609b-4373-8d26-a544cd12962b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "단어1 (단어1이 사용된 문장) vs 단어2 (단어2가 사용된 문장) -> 유사도\n",
      "\n",
      "배_1 (보기만 해도 배가 부르다.) vs 배_2 (점심을 먹지 못해 배가 많이 고팠다.) -> 0.7238\n",
      "배_1 (보기만 해도 배가 부르다.) vs 배_4 (그 섬에는 하루에 두 번씩 배가 들어온다.) -> 0.6015\n",
      "배_1 (보기만 해도 배가 부르다.) vs 배_6 (사각사각 씹히는 배의 맛이 달고 시원하다.) -> 0.5815\n",
      "배_1 (보기만 해도 배가 부르다.) vs 배_5 (나는 과일 중에서 배를 가장 좋아한다.) -> 0.5376\n",
      "배_1 (보기만 해도 배가 부르다.) vs 배_3 (배 한 척이 바다 한가운데 떠 있다.) -> 0.4839\n",
      "\n",
      "배_2 (점심을 먹지 못해 배가 많이 고팠다.) vs 배_1 (보기만 해도 배가 부르다.) -> 0.7238\n",
      "배_2 (점심을 먹지 못해 배가 많이 고팠다.) vs 배_5 (나는 과일 중에서 배를 가장 좋아한다.) -> 0.5074\n",
      "배_2 (점심을 먹지 못해 배가 많이 고팠다.) vs 배_4 (그 섬에는 하루에 두 번씩 배가 들어온다.) -> 0.5071\n",
      "배_2 (점심을 먹지 못해 배가 많이 고팠다.) vs 배_6 (사각사각 씹히는 배의 맛이 달고 시원하다.) -> 0.4337\n",
      "배_2 (점심을 먹지 못해 배가 많이 고팠다.) vs 배_3 (배 한 척이 바다 한가운데 떠 있다.) -> 0.4213\n",
      "\n",
      "배_3 (배 한 척이 바다 한가운데 떠 있다.) vs 배_4 (그 섬에는 하루에 두 번씩 배가 들어온다.) -> 0.7059\n",
      "배_3 (배 한 척이 바다 한가운데 떠 있다.) vs 배_6 (사각사각 씹히는 배의 맛이 달고 시원하다.) -> 0.6008\n",
      "배_3 (배 한 척이 바다 한가운데 떠 있다.) vs 배_5 (나는 과일 중에서 배를 가장 좋아한다.) -> 0.5345\n",
      "배_3 (배 한 척이 바다 한가운데 떠 있다.) vs 배_1 (보기만 해도 배가 부르다.) -> 0.4839\n",
      "배_3 (배 한 척이 바다 한가운데 떠 있다.) vs 배_2 (점심을 먹지 못해 배가 많이 고팠다.) -> 0.4213\n",
      "\n",
      "배_4 (그 섬에는 하루에 두 번씩 배가 들어온다.) vs 배_3 (배 한 척이 바다 한가운데 떠 있다.) -> 0.7059\n",
      "배_4 (그 섬에는 하루에 두 번씩 배가 들어온다.) vs 배_6 (사각사각 씹히는 배의 맛이 달고 시원하다.) -> 0.6077\n",
      "배_4 (그 섬에는 하루에 두 번씩 배가 들어온다.) vs 배_1 (보기만 해도 배가 부르다.) -> 0.6015\n",
      "배_4 (그 섬에는 하루에 두 번씩 배가 들어온다.) vs 배_5 (나는 과일 중에서 배를 가장 좋아한다.) -> 0.5753\n",
      "배_4 (그 섬에는 하루에 두 번씩 배가 들어온다.) vs 배_2 (점심을 먹지 못해 배가 많이 고팠다.) -> 0.5071\n",
      "\n",
      "배_5 (나는 과일 중에서 배를 가장 좋아한다.) vs 배_6 (사각사각 씹히는 배의 맛이 달고 시원하다.) -> 0.749\n",
      "배_5 (나는 과일 중에서 배를 가장 좋아한다.) vs 배_4 (그 섬에는 하루에 두 번씩 배가 들어온다.) -> 0.5753\n",
      "배_5 (나는 과일 중에서 배를 가장 좋아한다.) vs 배_1 (보기만 해도 배가 부르다.) -> 0.5376\n",
      "배_5 (나는 과일 중에서 배를 가장 좋아한다.) vs 배_3 (배 한 척이 바다 한가운데 떠 있다.) -> 0.5345\n",
      "배_5 (나는 과일 중에서 배를 가장 좋아한다.) vs 배_2 (점심을 먹지 못해 배가 많이 고팠다.) -> 0.5074\n",
      "\n",
      "배_6 (사각사각 씹히는 배의 맛이 달고 시원하다.) vs 배_5 (나는 과일 중에서 배를 가장 좋아한다.) -> 0.749\n",
      "배_6 (사각사각 씹히는 배의 맛이 달고 시원하다.) vs 배_4 (그 섬에는 하루에 두 번씩 배가 들어온다.) -> 0.6077\n",
      "배_6 (사각사각 씹히는 배의 맛이 달고 시원하다.) vs 배_3 (배 한 척이 바다 한가운데 떠 있다.) -> 0.6008\n",
      "배_6 (사각사각 씹히는 배의 맛이 달고 시원하다.) vs 배_1 (보기만 해도 배가 부르다.) -> 0.5815\n",
      "배_6 (사각사각 씹히는 배의 맛이 달고 시원하다.) vs 배_2 (점심을 먹지 못해 배가 많이 고팠다.) -> 0.4337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "if(__name__==\"__main__\"):\n",
    "    cache_dir = os.path.join(root_dir, \"cache\")\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "\n",
    "    config = {\n",
    "        \"data_path\": os.path.join(root_dir, \"datas.txt\"),\n",
    "        \"cache_dir_path\": cache_dir,\n",
    "        \"pretrained_model_name_or_path\": \"monologg/kobert\",\n",
    "        \"max_length\": 30\n",
    "    }\n",
    "\n",
    "    test(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8UiDVMc7r2r"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
